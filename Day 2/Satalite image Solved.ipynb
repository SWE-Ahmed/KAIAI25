{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification Using Logistic Regressoion\n",
    "In this assignment, you will perform multiclass classification using logistic regression. You will use the logistic regression library from the ``sklearn`` library. Some of the important libraries you 'may' use include the following:\n",
    "\n",
    "**LogisticRegression from sklearn.linear_model**\n",
    "\n",
    "**classification_report from sklearn.metrics**\n",
    "\n",
    "**confusion_matrix from sklearn.metrics**\n",
    "\n",
    "You can consult documentation at: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Information:\n",
    "\n",
    "The database consists of the multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number.\n",
    "\n",
    "The Landsat satellite data is one of the many sources of information available for a scene. The interpretation of a scene by integrating spatial data of diverse types and resolutions including multispectral and radar data, maps indicating topography, land use etc. is expected to assume significant importance with the onset of an era characterised by integrative approaches to remote sensing (for example, NASA's Earth Observing System commencing this decade). Existing statistical methods are ill-equipped for handling such diverse data types. Note that this is not true for Landsat MSS data considered in isolation (as in this sample database). This data satisfies the important requirements of being numerical and at a single resolution, and standard maximum-likelihood classification performs very well. Consequently, for this data, it should be interesting to compare the performance of other methods against the statistical approach.\n",
    "\n",
    "One frame of Landsat MSS imagery consists of four digital images of the same scene in different spectral bands. Two of these are in the visible region (corresponding approximately to green and red regions of the visible spectrum) and two are in the (near) infra-red. Each pixel is a 8-bit binary word, with 0 corresponding to black and 255 to white. The spatial resolution of a pixel is about 80m x 80m. Each image contains 2340 x 3380 such pixels.\n",
    "\n",
    "The database is a (tiny) sub-area of a scene, consisting of 82 x 100 pixels. Each line of data corresponds to a 3x3 square neighbourhood of pixels completely contained within the 82x100 sub-area. Each line contains the pixel values in the four spectral bands (converted to ASCII) of each of the 9 pixels in the 3x3 neighbourhood and a number indicating the classification label of the central pixel. The number is a code for the following classes:\n",
    "\n",
    "Number Class\n",
    "\n",
    "1 red soil\n",
    "\n",
    "2 cotton crop\n",
    "\n",
    "3 grey soil\n",
    "\n",
    "4 damp grey soil\n",
    "\n",
    "5 soil with vegetation stubble\n",
    "\n",
    "6 mixture class (all types present)\n",
    "\n",
    "7 very damp grey soil\n",
    "\n",
    "NB. There are no examples with class 6 in this dataset.\n",
    "\n",
    "In each line of data the four spectral values for the top-left pixel are given first followed by the four spectral values for the top-middle pixel and then those for the top-right pixel, and so on with the pixels read out in sequence left-to-right and top-to-bottom. Thus, the four spectral values for the central pixel are given by attributes 17,18,19 and 20. If you like you can use only these four attributes, while ignoring the others. This avoids the problem which arises when a 3x3 neighbourhood straddles a boundary.\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the data and split it into train/validate/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the file '186_satimage.csv' and split it into train/validate/test set with ratio ``70/15/15``\n",
    "\n",
    "**note: use random_state=777 whereever needed**\n",
    "\n",
    "**note: the first 36 column are features and the last column is the class label**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from sklearn.model_selection import train_test_split  # To split the dataset into training, validation, and testing sets\n",
    "import numpy as np  # For numerical operations (though not explicitly used here)\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # For evaluating model performance\n",
    "from sklearn.preprocessing import StandardScaler  # For standardizing the dataset\n",
    "from sklearn.linear_model import LogisticRegression  # For building the logistic regression model\n",
    "from sklearn.metrics import accuracy_score  # For calculating the model's accuracy\n",
    "\n",
    "# Loading the dataset from a CSV file\n",
    "data = pd.read_csv('186_satimage.csv')  # Replace with the correct path to your dataset if needed\n",
    "\n",
    "# Separating features (X) and target labels (y)\n",
    "X = data.iloc[:, :36]  # Selecting all rows and the first 36 columns as features\n",
    "y = data.iloc[:, 36]  # Selecting all rows and the 37th column as the target variable\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=777)\n",
    "\n",
    "# Further splitting the testing set into validation and testing subsets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=777)\n",
    "\n",
    "# Optional: Uncomment the line below to preview the first few rows of the dataset\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check the details of the features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some basic statistics of the features and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data=(6429, 37) (rows, columns)\n",
      "X_train=(4500, 36), X_val=(964, 36), X_test=(965, 36) (rows, columns)\n",
      "y_train=(4500,), y_val=(964,), y_test=(965,) (rows)\n",
      "\n",
      "Class % in y_train = [0.24 0.11 0.22 0.1  0.11 0.23]\n",
      "Class % in y_val = [0.25 0.11 0.2  0.1  0.11 0.24]\n",
      "Class % in y_test = [0.23 0.12 0.18 0.1  0.11 0.25]\n",
      "\n",
      "Feature-wise mean (X_train):\n",
      "0.117596     0.01\n",
      "1.241362     0.02\n",
      "1.184036     0.02\n",
      "0.815302     0.01\n",
      "-0.158561    0.01\n",
      "1.256483     0.01\n",
      "1.193546     0.02\n",
      "0.818486     0.01\n",
      "-0.141965    0.01\n",
      "0.879481     0.02\n",
      "0.67001      0.01\n",
      "0.40102      0.00\n",
      "0.05222      0.02\n",
      "1.204523     0.02\n",
      "1.181239     0.02\n",
      "0.758245     0.01\n",
      "-0.151111    0.02\n",
      "1.214967     0.02\n",
      "1.187378     0.02\n",
      "0.598708     0.01\n",
      "-0.136658    0.01\n",
      "1.01113      0.02\n",
      "0.899623     0.02\n",
      "0.761977     0.01\n",
      "-0.085593    0.02\n",
      "1.211546     0.02\n",
      "1.251179     0.02\n",
      "0.807707     0.01\n",
      "-0.069968    0.01\n",
      "1.21916      0.02\n",
      "1.250463     0.02\n",
      "0.597678     0.01\n",
      "-0.054291    0.01\n",
      "1.233342     0.02\n",
      "1.262255     0.02\n",
      "0.603258     0.01\n",
      "dtype: float64\n",
      "\n",
      "Feature-wise standard deviation (X_train):\n",
      "0.117596     1.00\n",
      "1.241362     1.00\n",
      "1.184036     1.00\n",
      "0.815302     0.99\n",
      "-0.158561    1.00\n",
      "1.256483     1.00\n",
      "1.193546     1.00\n",
      "0.818486     0.99\n",
      "-0.141965    1.00\n",
      "0.879481     1.00\n",
      "0.67001      1.00\n",
      "0.40102      0.99\n",
      "0.05222      1.00\n",
      "1.204523     1.00\n",
      "1.181239     0.99\n",
      "0.758245     0.99\n",
      "-0.151111    1.01\n",
      "1.214967     1.00\n",
      "1.187378     1.00\n",
      "0.598708     0.99\n",
      "-0.136658    1.01\n",
      "1.01113      1.00\n",
      "0.899623     1.00\n",
      "0.761977     0.99\n",
      "-0.085593    1.00\n",
      "1.211546     1.00\n",
      "1.251179     0.99\n",
      "0.807707     0.99\n",
      "-0.069968    1.00\n",
      "1.21916      1.00\n",
      "1.250463     0.99\n",
      "0.597678     0.99\n",
      "-0.054291    1.00\n",
      "1.233342     1.00\n",
      "1.262255     0.99\n",
      "0.603258     0.99\n",
      "dtype: float64\n",
      "\n",
      "Target class counts in training set:\n",
      "(array([1, 2, 3, 4, 5, 7]), array([1071,  484,  986,  430,  494, 1035]))\n"
     ]
    }
   ],
   "source": [
    "# Displaying the overall shape of the dataset\n",
    "print('X_data={} (rows, columns)'.format(data.shape))  # Total number of samples and features in the dataset\n",
    "\n",
    "# Displaying the shapes of training, validation, and test feature datasets\n",
    "print('X_train={}, X_val={}, X_test={} (rows, columns)'.format(X_train.shape, X_val.shape, X_test.shape))  # Number of samples in each dataset\n",
    "\n",
    "# Displaying the shapes of training, validation, and test target labels\n",
    "print('y_train={}, y_val={}, y_test={} (rows)'.format(y_train.shape, y_val.shape, y_test.shape))  # Number of target labels in each dataset\n",
    "print()\n",
    "\n",
    "# Calculating and displaying class distribution as percentages for the training set\n",
    "print('Class % in y_train = {}'.format((np.unique(y_train, return_counts=True)[1] / y_train.shape[0]).round(2)))  \n",
    "# Unique classes and their percentage distribution in the training set\n",
    "\n",
    "# Calculating and displaying class distribution as percentages for the validation set\n",
    "print('Class % in y_val = {}'.format((np.unique(y_val, return_counts=True)[1] / y_val.shape[0]).round(2)))  \n",
    "# Unique classes and their percentage distribution in the validation set\n",
    "\n",
    "# Calculating and displaying class distribution as percentages for the test set\n",
    "print('Class % in y_test = {}'.format((np.unique(y_test, return_counts=True)[1] / y_test.shape[0]).round(2)))  \n",
    "# Unique classes and their percentage distribution in the test set\n",
    "\n",
    "# Additional statistics for more insights:\n",
    "print()\n",
    "print('Feature-wise mean (X_train):\\n{}'.format(X_train.mean().round(2)))  # Mean of each feature in the training set\n",
    "print()\n",
    "print('Feature-wise standard deviation (X_train):\\n{}'.format(X_train.std().round(2)))  # Standard deviation of each feature in the training set\n",
    "print()\n",
    "print('Target class counts in training set:\\n{}'.format(np.unique(y_train, return_counts=True)))  # Count of each class in the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a logistic regression classifier and fine-tune the hyper parameters on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the fine tuning, report the best results on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Regression Validation Accuracy: 0.8568\n"
     ]
    }
   ],
   "source": [
    "# Standardize the training and validation datasets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Perform Logistic Regression with L2 penalty\n",
    "clf = LogisticRegression(\n",
    "    penalty='l2',  # L2 regularization\n",
    "    C=1.0,  # Regularization strength (default value)\n",
    "    max_iter=1500,  # Maximum number of iterations\n",
    "    random_state=777,  # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation data\n",
    "y_pred_on_val = clf.predict(X_val_scaled)\n",
    "\n",
    "# Calculate validation accuracy\n",
    "val_acc = accuracy_score(y_true=y_val, y_pred=y_pred_on_val)\n",
    "print(f\"L2 Regression Validation Accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Grid Search on hyperparameter\n",
    "\n",
    "this is just a simple way to try all the different hyperparameters combinations to find the best one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting penalty=l1, C=0.001, val_acc=0.5716\n",
      "Setting penalty=l1, C=0.01, val_acc=0.7593\n",
      "Setting penalty=l1, C=0.1, val_acc=0.8174\n",
      "Setting penalty=l1, C=1, val_acc=0.8371\n",
      "Setting penalty=l1, C=10, val_acc=0.8434\n",
      "Setting penalty=l1, C=100, val_acc=0.8413\n",
      "--------------------------------------------------\n",
      "Setting penalty=l2, C=0.001, val_acc=0.7459\n",
      "Setting penalty=l2, C=0.01, val_acc=0.7832\n",
      "Setting penalty=l2, C=0.1, val_acc=0.8216\n",
      "Setting penalty=l2, C=1, val_acc=0.8351\n",
      "Setting penalty=l2, C=10, val_acc=0.8423\n",
      "Setting penalty=l2, C=100, val_acc=0.8413\n",
      "--------------------------------------------------\n",
      "Setting penalty=elasticnet, C=0.001, val_acc=0.6784\n",
      "Setting penalty=elasticnet, C=0.01, val_acc=0.8164\n",
      "Setting penalty=elasticnet, C=0.1, val_acc=0.8371\n",
      "Setting penalty=elasticnet, C=1, val_acc=0.8568\n",
      "Setting penalty=elasticnet, C=10, val_acc=0.8537\n",
      "Setting penalty=elasticnet, C=100, val_acc=0.8496\n",
      "--------------------------------------------------\n",
      "Setting penalty=None, C=0.001, val_acc=0.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerk/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/zerk/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting penalty=None, C=0.01, val_acc=0.8517\n",
      "Setting penalty=None, C=0.1, val_acc=0.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerk/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting penalty=None, C=1, val_acc=0.8517\n",
      "Setting penalty=None, C=10, val_acc=0.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerk/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/zerk/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1186: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting penalty=None, C=100, val_acc=0.8517\n",
      "--------------------------------------------------\n",
      "Best setting penalty=elasticnet, C=1, val_acc=0.8568\n"
     ]
    }
   ],
   "source": [
    "# Define lists for penalties and regularization strengths (C values)\n",
    "penalty_list = ['l1', 'l2', 'elasticnet', None]\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Initialize variables to track the best hyperparameters and accuracy\n",
    "best_penalty = None\n",
    "best_C = None\n",
    "best_acc = 0.0\n",
    "\n",
    "# Loop over all penalty types and C values to find the best combination\n",
    "for penalty_val in penalty_list:\n",
    "    for C_val in C_values:\n",
    "        \n",
    "        # Choose solver based on the penalty type\n",
    "        solver = 'saga' if penalty_val == 'elasticnet' else 'lbfgs' if penalty_val == None else 'liblinear'\n",
    "\n",
    "        # Create a logistic regression model with the current hyperparameters\n",
    "        clf = LogisticRegression(\n",
    "            penalty=penalty_val,\n",
    "            C=C_val,\n",
    "            max_iter=1500,\n",
    "            random_state=777,\n",
    "            l1_ratio=0.5 if penalty_val == 'elasticnet' else None,  # Only used for elasticnet\n",
    "            solver=solver\n",
    "        )\n",
    "        \n",
    "        # Standardize features using StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Train the model and predict on validation set\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        y_pred_on_val = clf.predict(X_val_scaled)\n",
    "        \n",
    "        # Calculate validation accuracy\n",
    "        val_acc = accuracy_score(y_true=y_val, y_pred=y_pred_on_val)\n",
    "        \n",
    "        print(f\"Setting penalty={penalty_val}, C={C_val}, val_acc={val_acc:.4f}\")\n",
    "        \n",
    "        # Update best parameters if current accuracy is better\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_penalty = penalty_val\n",
    "            best_C = C_val\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Print the best hyperparameters and corresponding validation accuracy\n",
    "print(f\"Best setting penalty={best_penalty}, C={best_C}, val_acc={best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Do the final test on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the final scoring on the test set. Report different measure and show the confusion matrix. Record your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set= 0.8632\n",
      "--------------------------------------------------\n",
      "[[216   0   4   0   1   0]\n",
      " [  1 111   0   0   4   0]\n",
      " [  0   0 167   9   0   0]\n",
      " [  3   0  22  32   6  38]\n",
      " [  2   4   0   0  89  10]\n",
      " [  0   0   0  16  12 218]]\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.98      0.98       221\n",
      "           2       0.97      0.96      0.96       116\n",
      "           3       0.87      0.95      0.91       176\n",
      "           4       0.56      0.32      0.41       101\n",
      "           5       0.79      0.85      0.82       105\n",
      "           7       0.82      0.89      0.85       246\n",
      "\n",
      "    accuracy                           0.86       965\n",
      "   macro avg       0.83      0.82      0.82       965\n",
      "weighted avg       0.85      0.86      0.85       965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(\n",
    "        penalty='elasticnet',\n",
    "        C=1,\n",
    "        max_iter=1000,\n",
    "        random_state=777,\n",
    "        l1_ratio=0.5,\n",
    "        solver='saga'\n",
    "    )\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_on_test = clf.predict(X_test_scaled)\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=y_pred_on_test)\n",
    "\n",
    "print(f\"Accuracy on test set= {test_acc:.4f}\")\n",
    "\n",
    "matrix = confusion_matrix(y_test,y_pred_on_test)\n",
    "print(\"-\"*50)\n",
    "print(matrix)\n",
    "\n",
    "class_report = classification_report(y_test,y_pred_on_test)\n",
    "print(\"-\"*50)\n",
    "print(class_report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
